---
phase: 08-p3-optimization-hardening
plan: 04
type: execute
wave: 2
depends_on: []
files_modified:
  - src/Bytes.h
  - src/Bytes.cpp
  - src/Packet.h
  - src/ObjectPool.h
autonomous: true

must_haves:
  truths:
    - "Bytes COW copy can use pool allocator for common buffer sizes"
    - "Packet::Object fixed-size fields use inline buffers"
    - "Pool exhaustion falls back to heap gracefully"
    - "Pool allocator is thread-safe for multi-task access"
  artifacts:
    - path: "src/ObjectPool.h"
      provides: "Generic fixed-size object pool with freelist"
      contains: "template.*ObjectPool"
    - path: "src/Bytes.h"
      provides: "Pool-backed allocation option"
      pattern: "ObjectPool|pool"
    - path: "src/Packet.h"
      provides: "Inline buffers for fixed-size fields"
      pattern: "uint8_t _packet_hash\\[|inline.*buffer"
  key_links:
    - from: "src/Bytes.cpp"
      to: "src/ObjectPool.h"
      via: "Pool allocation in newData/exclusiveData"
      pattern: "pool.*allocate"
---

<objective>
Implement pool allocators for Bytes buffers and Packet objects, plus inline buffers for Packet fixed-size fields.

Purpose: Complete MEM-H1 (Bytes COW pool), MEM-H2 (Packet pool), and MEM-H3 (Packet inline buffers) to reduce per-packet heap fragmentation.

Output: Hot-path allocations use fixed-size pools with heap fallback. Packet fixed-size fields save ~96-150 bytes per packet.
</objective>

<execution_context>
@/home/tyler/.claude/get-shit-done/workflows/execute-plan.md
@/home/tyler/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-p3-optimization-hardening/08-RESEARCH.md

# Source files to modify
@src/Bytes.h
@src/Bytes.cpp
@src/Packet.h
@src/Type.h (for buffer size constants)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ObjectPool template class</name>
  <files>src/ObjectPool.h</files>
  <action>
    Create a new header file `src/ObjectPool.h` with a thread-safe fixed-size object pool:

    ```cpp
    #pragma once

    #include <cstddef>
    #include <cstdint>
    #include <new>
    #include "freertos/FreeRTOS.h"
    #include "freertos/portmacro.h"

    namespace RNS {

    /**
     * Fixed-size object pool with O(1) allocate/deallocate.
     * Thread-safe via spinlock. Falls back to nullptr on exhaustion.
     *
     * Template parameters:
     *   T - Object type to pool
     *   N - Pool capacity (number of slots)
     *
     * Usage:
     *   ObjectPool<MyClass, 16> pool;
     *   MyClass* obj = pool.allocate();  // nullptr if exhausted
     *   pool.deallocate(obj);
     */
    template <typename T, size_t N>
    class ObjectPool {
    public:
        ObjectPool() : _first_free(0), _allocated_count(0) {
            portMUX_INITIALIZE(&_mux);
            // Initialize freelist chain
            for (size_t i = 0; i < N - 1; i++) {
                _slots[i].next_free = i + 1;
            }
            _slots[N - 1].next_free = INVALID_SLOT;
        }

        /**
         * Allocate object from pool.
         * Returns nullptr if pool exhausted (caller should fall back to heap).
         * Thread-safe.
         */
        T* allocate() {
            portENTER_CRITICAL(&_mux);
            if (_first_free == INVALID_SLOT) {
                portEXIT_CRITICAL(&_mux);
                return nullptr;  // Pool exhausted
            }
            size_t slot = _first_free;
            _first_free = _slots[slot].next_free;
            _allocated_count++;
            portEXIT_CRITICAL(&_mux);

            // Placement new to construct object
            return new (&_slots[slot].storage) T();
        }

        /**
         * Return object to pool.
         * Pointer must have been obtained from this pool's allocate().
         * Thread-safe.
         */
        void deallocate(T* ptr) {
            if (!ptr) return;

            // Calculate slot index from pointer
            uintptr_t offset = reinterpret_cast<uintptr_t>(ptr) -
                              reinterpret_cast<uintptr_t>(_slots);
            size_t slot = offset / sizeof(Slot);

            if (slot >= N) {
                // Not from this pool - ignore (caller's responsibility)
                return;
            }

            // Explicit destructor call
            ptr->~T();

            portENTER_CRITICAL(&_mux);
            _slots[slot].next_free = _first_free;
            _first_free = slot;
            _allocated_count--;
            portEXIT_CRITICAL(&_mux);
        }

        /**
         * Check if pointer was allocated from this pool.
         */
        bool owns(T* ptr) const {
            if (!ptr) return false;
            uintptr_t start = reinterpret_cast<uintptr_t>(_slots);
            uintptr_t end = start + sizeof(_slots);
            uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);
            return addr >= start && addr < end;
        }

        size_t allocated() const { return _allocated_count; }
        size_t capacity() const { return N; }
        size_t available() const { return N - _allocated_count; }

    private:
        static constexpr size_t INVALID_SLOT = ~size_t(0);

        struct Slot {
            union {
                alignas(T) char storage[sizeof(T)];
                size_t next_free;
            };
        };

        Slot _slots[N];
        size_t _first_free;
        size_t _allocated_count;
        portMUX_TYPE _mux;
    };

    } // namespace RNS
    ```

    Include guards against non-FreeRTOS builds (native) with preprocessor checks.
  </action>
  <verify>
    File exists and compiles: `pio run -e native` (may need stub for portMUX on native).
  </verify>
  <done>
    ObjectPool.h provides thread-safe fixed-size pool with freelist.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add inline buffers to Packet::Object</name>
  <files>src/Packet.h</files>
  <action>
    Modify the Packet::Object class (around line 283) to use inline buffers for fixed-size fields:

    Replace:
    ```cpp
    Bytes _packet_hash;
    Bytes _ratchet_id;
    Bytes _destination_hash;
    Bytes _transport_id;
    ```

    With:
    ```cpp
    // Inline buffers for fixed-size fields (saves ~96 bytes per packet)
    // These fields have known maximum sizes from Type.h constants
    uint8_t _packet_hash_buf[32];       // SHA256 hash
    uint8_t _packet_hash_len = 0;

    uint8_t _ratchet_id_buf[32];        // Ratchet ID
    uint8_t _ratchet_id_len = 0;

    uint8_t _destination_hash_buf[16];  // Type::Reticulum::DESTINATION_LENGTH
    uint8_t _destination_hash_len = 0;

    uint8_t _transport_id_buf[16];      // Type::Reticulum::DESTINATION_LENGTH
    uint8_t _transport_id_len = 0;
    ```

    Add helper methods to the Packet class to get/set these as Bytes views:
    ```cpp
    // Inline buffer accessors - return Bytes view without allocation
    inline Bytes packet_hash() const {
        assert(_object);
        if (_object->_packet_hash_len == 0) return Bytes();
        return Bytes(_object->_packet_hash_buf, _object->_packet_hash_len);
    }
    inline void packet_hash(const Bytes& hash) {
        assert(_object);
        size_t len = std::min(hash.size(), sizeof(_object->_packet_hash_buf));
        memcpy(_object->_packet_hash_buf, hash.data(), len);
        _object->_packet_hash_len = len;
    }
    ```

    Repeat pattern for ratchet_id, destination_hash, transport_id.

    Keep the variable-size Bytes members unchanged:
    - _raw, _data, _plaintext, _header, _ciphertext
  </action>
  <verify>
    1. Packet.h compiles without errors
    2. Grep confirms inline buffers: `grep "_packet_hash_buf" src/Packet.h`
    3. No remaining `Bytes _packet_hash;` line
  </verify>
  <done>
    Packet::Object uses inline buffers for 4 fixed-size fields.
    Saves ~96 bytes overhead per packet (4 fields x 24 bytes Bytes overhead).
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate pool allocator for Bytes COW</name>
  <files>src/Bytes.h, src/Bytes.cpp</files>
  <action>
    This task adds optional pool backing for Bytes Data allocation.

    In src/Bytes.h:
    1. Add forward declaration and static pool for common buffer sizes
    2. The pool should be for the underlying Data vector, not Bytes objects themselves

    NOTE: This is the most complex task. The approach should be:
    - Keep existing PSRAMAllocator-backed Data as default
    - Pool is used for COW copies in exclusiveData() for common sizes
    - If pool exhausted, fall back to normal heap allocation

    Due to complexity, a simpler initial approach:
    1. Create a pool for Bytes::Data vectors of common sizes (256, 512 bytes)
    2. In exclusiveData() COW path, try pool first, fall back to make_shared

    Implementation guidance:
    - Pooling std::vector is complex due to variable size
    - Alternative: Pool fixed-size byte arrays for common packet sizes
    - Or: Skip this task if complexity is too high, document as future optimization

    If implementing fully:
    ```cpp
    // In Bytes.h, after class definition
    namespace BytesPool {
        // Pool for 256-byte buffers (common packet data size)
        extern ObjectPool<std::array<uint8_t, 256>, 32> small_buffer_pool;
        // Pool for 512-byte buffers
        extern ObjectPool<std::array<uint8_t, 512>, 16> medium_buffer_pool;
    }
    ```

    If deferring:
    - Add comment noting pool integration as future optimization
    - The inline buffer work in Task 2 provides most of the benefit
  </action>
  <verify>
    If implemented: Pool allocation path exists in Bytes.cpp.
    If deferred: Comment documents future optimization opportunity.
    Build succeeds either way.
  </verify>
  <done>
    Either:
    - Bytes COW uses pool allocator for common sizes with heap fallback, OR
    - Future optimization documented (inline buffers in Task 2 provide majority of savings)
  </done>
</task>

</tasks>

<verification>
1. `ls src/ObjectPool.h` confirms pool template exists
2. `grep "_packet_hash_buf" src/Packet.h` confirms inline buffers
3. Native build compiles: `pio run -e native`
4. T-Deck build compiles (if environment available): `pio run -e tdeck`
</verification>

<success_criteria>
- MEM-H3 complete: Packet inline buffers save ~96 bytes per packet
- MEM-H2 partial: ObjectPool template created for future Packet::Object pooling
- MEM-H1 partial or deferred: Bytes pool documented or implemented
- Thread-safe pool implementation with spinlock protection
</success_criteria>

<output>
After completion, create `.planning/phases/08-p3-optimization-hardening/08-04-SUMMARY.md`
</output>
